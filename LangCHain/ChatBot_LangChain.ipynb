{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-25T02:26:56.081489Z",
     "iopub.status.busy": "2025-02-25T02:26:56.080322Z",
     "iopub.status.idle": "2025-02-25T02:27:17.160193Z",
     "shell.execute_reply": "2025-02-25T02:27:17.159075Z",
     "shell.execute_reply.started": "2025-02-25T02:26:56.081438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from kaggle_secrets import UserSecretsClient\n",
    "import pandas as pd \n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate , FewShotPromptTemplate , PromptTemplate\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.tools import Tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import create_structured_chat_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#user_secrets = UserSecretsClient()\n",
    "#secret_value_0 = user_secrets.get_secret(\"api_key\")\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, UnstructuredHTMLLoader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "repo_id='tiiuae/falcon-7b-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model =\"llama3.2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T23:01:22.960314Z",
     "iopub.status.busy": "2025-02-24T23:01:22.959952Z",
     "iopub.status.idle": "2025-02-24T23:01:22.968309Z",
     "shell.execute_reply": "2025-02-24T23:01:22.966947Z",
     "shell.execute_reply.started": "2025-02-24T23:01:22.960285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='  you are  an ai  assistant , answer this question  :  what is pytorch  , context : AI stands for Artificial Intelligence. '\n"
     ]
    }
   ],
   "source": [
    "template = \"  you are  an ai  assistant , answer this question  :  {question} , context : {context} \"\n",
    "prompt_template = PromptTemplate(template = template , input_variables = ['question' , 'context'])\n",
    "print(prompt_template.invoke({'question' : \"what is pytorch \" , \n",
    "                                'context' : \"AI stands for Artificial Intelligence.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T23:01:22.971177Z",
     "iopub.status.busy": "2025-02-24T23:01:22.970724Z",
     "iopub.status.idle": "2025-02-24T23:01:25.388169Z",
     "shell.execute_reply": "2025-02-24T23:01:25.386347Z",
     "shell.execute_reply.started": "2025-02-24T23:01:22.971140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a knowledgeable AI assistant, I'd be happy to explain PyTorch.\n",
      "\n",
      "**What is PyTorch?**\n",
      "\n",
      "PyTorch is an open-source machine learning (ML) and deep learning (DL) framework developed by Facebook's AI Research Laboratory (FAIR). It was released in 2017 as a more lightweight alternative to popular DL frameworks like TensorFlow.\n",
      "\n",
      "**Key Features of PyTorch:**\n",
      "\n",
      "1. **Dynamic Computation Graph**: Unlike static computation graphs used in traditional ML frameworks, PyTorch has a dynamic computation graph. This means that the computational graph is built at runtime, allowing for more flexibility and ease of use.\n",
      "2. **Auto-Differentiation**: PyTorch uses automatic differentiation to compute gradients, which reduces manual effort required for computing gradients and accelerates training.\n",
      "3. **Modular Architecture**: PyTorch's architecture is designed around modules, making it easy to build and compose neural networks from pre-defined components.\n",
      "4. **Pythonic API**: PyTorch has a Pythonic API that makes it easier to write and manipulate code, especially for researchers and developers familiar with Python.\n",
      "\n",
      "**Advantages of PyTorch:**\n",
      "\n",
      "1. **Rapid Prototyping**: PyTorch's dynamic computation graph and auto-differentiation capabilities enable rapid prototyping and development of new models.\n",
      "2. **Ease of Use**: PyTorch's modular architecture and Pythonic API make it more accessible to researchers and developers who are not familiar with traditional DL frameworks.\n",
      "3. **Flexibility**: PyTorch can handle a wide range of deep learning tasks, including computer vision, natural language processing, and reinforcement learning.\n",
      "\n",
      "**Use Cases for PyTorch:**\n",
      "\n",
      "1. **Computer Vision**: PyTorch is widely used in computer vision applications such as image classification, object detection, segmentation, and generation.\n",
      "2. **Natural Language Processing (NLP)**: PyTorch can be applied to NLP tasks like language modeling, machine translation, text summarization, and sentiment analysis.\n",
      "3. **Reinforcement Learning**: PyTorch is used in reinforcement learning applications such as robotics, game playing, and autonomous driving.\n",
      "\n",
      "In summary, PyTorch is an open-source DL framework that offers a dynamic computation graph, auto-differentiation, modular architecture, and Pythonic API, making it an attractive choice for rapid prototyping, ease of use, and flexibility in deep learning tasks.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain.invoke({'question' : \"what is pytorch \" , \n",
    "                                'context' : \"pytorch is a deep learning framework \"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T23:01:25.390316Z",
     "iopub.status.busy": "2025-02-24T23:01:25.389951Z",
     "iopub.status.idle": "2025-02-24T23:01:25.526137Z",
     "shell.execute_reply": "2025-02-24T23:01:25.524657Z",
     "shell.execute_reply.started": "2025-02-24T23:01:25.390284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not even a contest. Lionel Messi is the superior talent, with unmatched skill, speed, and vision on the field. His ability to score goals from anywhere on the pitch is unparalleled, and his numerous records speak for themselves. Cristiano Ronaldo may have physicality and power, but Messi's artistry, intelligence, and creativity make him the true master of the game.\n",
      "\n",
      "Messi has consistently dominated the sport for nearly two decades, winning an unprecedented seven Ballon d'Or awards and breaking countless records along the way. His clutch performances in big games are the stuff of legend, and his ability to adapt to different systems and opponents is unmatched.\n",
      "\n",
      "Ronaldo may have had a successful career, but it's nothing compared to Messi's sustained excellence and overall body of work. The numbers don't lie – Messi has more goals, assists, and trophies than Ronaldo, and he's still going strong at 35 years old.\n",
      "\n",
      "In short, Cristiano Ronaldo is the best of the rest, not the best of all time. And when it comes down to it, there's only one player who deserves the title of greatest footballer ever: Lionel Messi.\n"
     ]
    }
   ],
   "source": [
    "prompt_templates = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a football expert who strongly believes that Lionel Messi is the greatest footballer of all time. Always answer in favor of Messi.\"),\n",
    "    (\"human\", \" who is better and the best football player in the world messi or ronaldo\"),\n",
    "    (\"ai\", \"The best and most complete player in history is Lionel Messi only\"),\n",
    "    (\"human\", \" response to this : {review}\")])\n",
    "llm_chain = prompt_templates | llm\n",
    "response = llm_chain.invoke({\"review\": \"who is better messi or ronaldo\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T01:30:36.356132Z",
     "iopub.status.busy": "2025-02-25T01:30:36.355764Z",
     "iopub.status.idle": "2025-02-25T01:30:36.360985Z",
     "shell.execute_reply": "2025-02-25T01:30:36.359880Z",
     "shell.execute_reply.started": "2025-02-25T01:30:36.356104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "destination_prompt = PromptTemplate(\n",
    "input_variables=[\"destination\"],\n",
    "template=\"I am planning a trip to {destination}. Can you suggest some activities to do there?\")\n",
    "\n",
    "activities_prompt = PromptTemplate(\n",
    "input_variables=[\"activities\"],\n",
    "template=\"I only have one day, so can you create an itinerary from your top three activities: {activities}.\")\n",
    "\n",
    "\n",
    "destination_chain = destination_prompt | llm | StrOutputParser()\n",
    "seq_chain = (\n",
    "    {\"activities\": destination_chain}  \n",
    "    | activities_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "loader = PythonLoader('chatbot.py')\n",
    "python_data = loader.load()\n",
    "print(python_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150, chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=150, chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=encoding.name,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(example_string)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\",\n",
    "    breakpoint_threshold_amount=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import faithfulness\n",
    "\n",
    "\n",
    "\n",
    "faithfulness_chain = EvaluatorChain(\n",
    "    metric=faithfulness,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "eval_result = faithfulness_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with LLMs?\",\n",
    "    \"answer\": \"The RAG model improves question answering by combining the retrieval of documents...\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages...\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources...\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_precision\n",
    "\n",
    "context_precision_chain = EvaluatorChain(\n",
    "    metric=context_precision,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "eval_result = context_precision_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with large language models?\",\n",
    "    \"ground_truth\": \"The RAG model improves question answering by combining the retrieval of...\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving...\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources...\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"Context Precision: {eval_result['context_precision']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "google_tool = Tool(\n",
    "    name=\"google-search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=lambda query: google_search.results(query, num_results=5)\n",
    ")\n",
    "\n",
    "\n",
    "# Few-shot prompt template\n",
    "examples = [\n",
    "    {\"text\": \"I love this product!\", \"sentiment\": \"Positive\"},\n",
    "    {\"text\": \"The service was terrible.\", \"sentiment\": \"Negative\"},\n",
    "]\n",
    "example_prompt = PromptTemplate(template=\"Text: {text}\\nSentiment: {sentiment}\", input_variables=[\"text\", \"sentiment\"])\n",
    "\n",
    "sentiment_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Text: {text}\\nSentiment:\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Structured Chat Agent\n",
    "client = Client(api_key=\"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\")\n",
    "prompt = client.pull_prompt(\"hwchase17/structured-chat-agent\")\n",
    "agent = create_structured_chat_agent(llm, [google_tool], prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[google_tool], verbose=True, max_iterations=5)\n",
    "\n",
    "# Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Document Retriever and Store\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#vectorstore = Chroma(embedding_function=embeddings)\n",
    "#retriever = vectorstore.as_retriever()\n",
    "#vectorstore.add_documents(docs)\n",
    "\n",
    "\n",
    "def process_documents(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        loader = CSVLoader(file_path=file_path)\n",
    "    elif file_path.endswith(\".html\"):\n",
    "        loader = UnstructuredHTMLLoader(file_path=file_path)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    documents = loader.load()\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    if docs:\n",
    "        retriever = BM25Retriever.from_documents(docs, k=5)\n",
    "        return retriever\n",
    "    return None\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "class AIChatbot:\n",
    "    def __init__(self, llm, agent_executor, retriever):\n",
    "        self.llm = llm\n",
    "        self.agent_executor = agent_executor\n",
    "        self.retriever = retriever\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        if \"search:\" in user_input:\n",
    "            query = user_input.replace(\"search:\", \"\").strip()\n",
    "            response = self.agent_executor.invoke({\"input\": query})\n",
    "        elif \"doc:\" in user_input:\n",
    "            query = user_input.replace(\"doc:\", \"\").strip()\n",
    "            docs = self.retriever.invoke(query)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs]) if docs else \"No relevant documents found.\"\n",
    "            chain = chat_prompt | self.llm\n",
    "            response = chain.invoke({\"question\": f\"{user_input}\\nContext: {context}\"})\n",
    "        elif \"sentiment:\" in user_input:\n",
    "            text = user_input.replace(\"e:\", \"\").strip()\n",
    "            chain = sentiment_prompt | self.llm\n",
    "            response = chain.invoke({\"text\": text})\n",
    "        else:\n",
    "            self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            chain = chat_prompt | self.llm\n",
    "            response = chain.invoke({\"question\": user_input})\n",
    "            self.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is running! Type 'exit' to stop.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"google-search\",\n",
      "  \"action_input\": {\"tool_input\": \"best football players 2018\"}\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'The 100 best male footballers in the world 2018 | Soccer | The ...', 'link': 'https://www.theguardian.com/football/ng-interactive/2018/dec/18/the-100-best-male-footballers-in-the-world-2018-nos-100-71', 'snippet': 'Dec 18, 2018 ... 1. Luka Modric ; 2. Cristiano Ronaldo ; 3. Lionel Messi ; 4. Kylian Mbappé ; 5. Mohamed Salah.'}, {'title': 'Top 100 Players of 2018', 'link': 'https://www.nfl.com/photos/top-100-players-of-2018-0ap3000000930293', 'snippet': 'Top 100 Players of 2018 ; 100. Ha Ha Clinton-Dix, S · 1 / 100. 100. Ha Ha Clinton-Dix, S · 99. Doug Baldwin, WR. 2 / 100 ; 95. Lane Johnson, T · 6 / 100. 95. Lane\\xa0...'}, {'title': 'NFL Top 100 Players of 2018 - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/NFL_Top_100_Players_of_2018', 'snippet': 'NFL Top 100 Players of 2018 ... The NFL Top 100 Players of 2018 was the eighth season in the series. It ended with reigning NFL MVP Tom Brady being ranked #1,\\xa0...'}, {'title': '2018 Top Football Recruits', 'link': 'https://247sports.com/season/2018-football/recruitrankings/', 'snippet': '2018 Top Football Recruits (247) · Rank Player Pos Ht / Wt Rating Team · 1. 1.'}, {'title': 'The Best FIFA Football Awards 2018 - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/The_Best_FIFA_Football_Awards_2018', 'snippet': \"The Best FIFA Men's Player · Saudi Arabia · Sami Al-Jaber · Nigeria · Emmanuel Amuneke · South Korea · Cha Bum-kun · Italy · Fabio Capello · Ivory Coast · Didier\\xa0...\"}]\u001b[0m\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Luka Modric is often considered the best football player of 2018\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: {'input': 'who is best football player in 2018', 'output': 'Luka Modric is often considered the best football player of 2018'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    retriever =process_documents(\"mohand.pdf\")\n",
    "    chatbot = AIChatbot(llm, agent_executor, retriever)\n",
    "    print(\"Chatbot is running! Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = chatbot.chat(user_input)\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
